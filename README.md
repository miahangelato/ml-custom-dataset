# ğŸ“Š Activity 2: Build a Custom Dataset and Train a Classifier

## ğŸ•’ Time: ~2 hours

---

## ğŸ¯ Objectives

- Create your own labeled dataset and save it as a `.csv` file
- Load and visualize your dataset using Python
- Train a classifier using `scikit-learn`
- Test predictions on custom data
- (Optional) Wrap it into a Django API

---

## ğŸ’» Project Folder Structure

```
ml-custom-dataset/
â”œâ”€â”€ my_dataset.csv                  <-- Your custom dataset (you'll create this)
â”œâ”€â”€ train_model.py                  <-- Loads, visualizes, trains model
â”œâ”€â”€ predict.py                      <-- Separate script to test predictions
â”œâ”€â”€ requirements.txt                <-- Dependencies list
â””â”€â”€ README.md                       
```

---

## ğŸ“¦ Setup Instructions

### 1. Create Your Project Folder

```bash
mkdir ml-custom-dataset
cd ml-custom-dataset
```

---

### 2. Create Your Dataset (CSV)

Open Excel or Google Sheets. Make a dataset with at least 2 numeric input features and 1 label.

Example:

```
petal_length,petal_width,species
1.4,0.2,setosa
4.7,1.4,versicolor
5.5,2.1,virginica
...
```

- Save/export as `my_dataset.csv` in your project folder.
- Recommended size: 30â€“50 rows.

---

### 3. Create `requirements.txt`

```txt
pandas
matplotlib
seaborn
scikit-learn
```

Install all at once:

```bash
pip install -r requirements.txt
```

---

### 4. Create `train_model.py`

This script loads your CSV, visualizes the data, and trains a model.

```python
# train_model.py

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
import joblib

# 1. Load dataset
df = pd.read_csv("my_dataset.csv")
print("First 5 rows:")
print(df.head())

# 2. Visualize (change x/y if you use different columns)
sns.scatterplot(data=df, x="petal_length", y="petal_width", hue="species")
plt.title("Custom Dataset")
plt.show()

# 3. Prepare features and labels
X = df[["petal_length", "petal_width"]]
le = LabelEncoder()
y = le.fit_transform(df["species"])

# 4. Train model
model = RandomForestClassifier()
model.fit(X, y)

# 5. Save model and encoder for later use
joblib.dump(model, "model.pkl")
joblib.dump(le, "label_encoder.pkl")

print("âœ… Model trained and saved as model.pkl")
```

---

### 5. Create `predict.py`

This file loads the trained model and predicts new data.

```python
# predict.py

import joblib

# Load saved model and label encoder
model = joblib.load("model.pkl")
le = joblib.load("label_encoder.pkl")

# Test input (edit this!)
sample = [[5.1, 1.8]]  # petal_length, petal_width

# Predict
pred = model.predict(sample)
label = le.inverse_transform(pred)

print("Prediction:", label[0])
```

---

## ğŸ”¥ (Optional) API Extension

Already familiar with Django from Activity 1?  
Recreate a `/predict/` endpoint using your trained `model.pkl`.

---

## ğŸ“„ Final Report Format

### âœ… Submit: GitHub Repo + Screenshot Folder or README

**Repo name:** `ml-custom-dataset`

**Expected Files:**

| File                     | Description                          |
|--------------------------|--------------------------------------|
| `my_dataset.csv`         | Your custom dataset                  |
| `train_model.py`         | Trains and saves model               |
| `predict.py`             | Loads and predicts new input         |
| `README.md`              | Final writeup and summary            |
| `report/` folder         | Screenshots (optional)               |

---

### ğŸ“· Screenshot Requirements

| Screenshot Topic              | Description                                 |
|-------------------------------|---------------------------------------------|
| 1. Raw dataset                | CSV file shown in Excel or Sheets           |
| 2. pandas preview             | `print(df.head())` from `train_model.py`    |
| 3. Visualization              | Scatterplot or seaborn output               |
| 4. Training output            | CLI print confirming model was trained      |
| 5â€“7. Sample predictions       | Console printouts from `predict.py`         |
| 8â€“10. (Optional) Postman/API  | If API was added                           |

---

### ğŸ“ `README.md` Should Include:

- âœ… Brief explanation of your dataset (what & why)
- âœ… Features and label used
- âœ… Classifier used (e.g., RandomForest)
- âœ… At least 2 sample predictions
- âœ… How to run your code

---

### ğŸ§  Reflection Questions (Add to README)

- Why did you choose this dataset?
- What do you think affects prediction accuracy?
- How could you improve this in the future?

---

## ğŸ’¡ Bonus Ideas (Optional for +5 pts)

- Add a third feature (3D scatter plot!)
- Export predictions to a new CSV
- Compare accuracy between different classifiers (e.g., KNN vs RF)

---

## âœ… Grading Guide

| Criteria                             | Points |
|--------------------------------------|--------|
| Dataset Created and Loaded Correctly | 20     |
| Visualization with Plot              | 20     |
| Model Training                       | 20     |
| Sample Predictions                   | 20     |
| Organized Repo + Report              | 20     |
| **TOTAL**                            | **100**|

---

## ğŸ‰ Congratulations!

You've now created your own dataset, trained a model, made predictions, and optionally exposed it as an API â€” just like real-world data science!
